{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6425d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0f72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b24406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5163d33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.fromarray(train_images[0])\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae70c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# model = keras.Sequential([\n",
    "#                             layers.Dense(512, activation=\"relu\"),\n",
    "#                             layers.Dense(10, activation=\"softmax\")\n",
    "#                          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc8199",
   "metadata": {},
   "source": [
    "# Another way to create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea0f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models,layers\n",
    "\n",
    "model = models.Sequential()   \n",
    "\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c67307",
   "metadata": {},
   "source": [
    "# Data must be in ths shape as the the model required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6bfd140",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images= train_images.reshape((60000, 784))\n",
    "                                    \n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "\n",
    "test_images = test_images.reshape((10000, 784))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ce21b",
   "metadata": {},
   "source": [
    "# Before fitting model must be compiled \n",
    "   - if we are enocding our data using Label Encoder, we will have to use 'sparse_categorical_crossentropy'\n",
    "   - if we are encoding our data using One Hot Encoder,We will have to use 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0ef06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# train_labels= to_categorical(train_labels)\n",
    "test_labels_by_cat  = to_categorical(test_labels)\n",
    "train_labels_by_cat  = to_categorical(train_labels)\n",
    "train_labels_by_cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41988757",
   "metadata": {},
   "source": [
    "In simple english, your loss function should be categorical_crossentropy if you have applied labelbinarizer (for hot encoding) to your test data. If you have not hot encoded your test data, you should use'sparse_categorical_crossentropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7505bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9533f66",
   "metadata": {},
   "source": [
    "# Fitting / Trainnig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d30d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2658 - accuracy: 0.9234\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1083 - accuracy: 0.9683\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0715 - accuracy: 0.9788\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0518 - accuracy: 0.9846\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0391 - accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b1080a6a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels_by_cat, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1079ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719524d4",
   "metadata": {},
   "source": [
    "# Model Layers Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e11cadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b6fb332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0670 - accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08977c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(model.predict(test_images[[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8930becf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a37529c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABJUlEQVR4nL3RzXLTQBhE0aP50Xgky5adEEPx/s/Ggk1IOaQqAQ8LbCpS9vT2Vvf09MdNoU+ZhMIxeq/tgPutqcTOaKksPDBQMfq0pCdCwmQqPuqu2JU9QloZ/wYe9faE9Zs/2vd2eWkelHRUl/RAUV2aQLdua6bqL9mU7Vc0c0Qb5/ShazIVZWovAmVasLiJcvz11OzyrF8ae1p7bq/PTVQtk0MRSCE/fomi1UjpFDZJ0bBb5VYyg9xsXTcKqlkfBfuL1Hk7j2f3N8uGkVbKZ1+Z/XbsmEFnkJLH86j0Nt4aYTtfW+q2Nmk8/GzG3mtTT+LtKzuxU+m0b601op3D+83jLOTpQLxTU57+jTCPUlBV4oEB8xWGRO7GIBo6+mTbra75X/UHhrsy11Nqn2EAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three = Image.open('three.jpeg')\n",
    "three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e2e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = three.convert(\"L\")\n",
    "\n",
    "# Save the grayscale image\n",
    "image.save(\"grayscale_three.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb1f2b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eight_grey = Image.open('grayscale_three.jpg')\n",
    "arr = np.asarray(eight_grey)\n",
    "arr = arr.reshape((1,784))\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1d9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eight = Image.open('three.jpeg')\n",
    "image.save(\"grayscale_three.jpg\")\n",
    "\n",
    "np.argmax(model.predict(np.asarray(Image.open('grayscale_three.jpg')).reshape(1,784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a04c0fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5424ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39d4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727c9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4daec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.Sequential()\n",
    "from tensorflow.keras.layers import Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "149b0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data  = pd.DataFrame({'colors':['Red','Green','Blue','Green','Blue','Red','Green','Red']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3db07ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Red', 'Green', 'Blue'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.colors.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f7947b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colors_Blue</th>\n",
       "      <th>colors_Green</th>\n",
       "      <th>colors_Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colors_Blue  colors_Green  colors_Red\n",
       "0            0             0           1\n",
       "1            0             1           0\n",
       "2            1             0           0\n",
       "3            0             1           0\n",
       "4            1             0           0\n",
       "5            0             0           1\n",
       "6            0             1           0\n",
       "7            0             0           1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33ae1354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABr0lEQVR4nN1VwXLFIAgExOf/f23GZ8AedkKtqUlej/WQiQZhYRfCvXd6sPZ9V9XeOzPjxMxSSu4uIpPxvB+XmRGRuxORqpoZ3vFMKZmZiLj7hIxXSAMUsAAXzuNroH6KlJmBFE94dHf4EpEw+B3Rarl7a+31esESfscXVXX3fd+ni0unMFVVoB7ZEJExEmo6Ll2lD+tgxt3NLOdMRLVWEck5B/WTBpZEwQ5UhA1ixH2wz8yj1IhoifSsvt47DhHD3VNKoxjunYJoERERKD/c4T7qMwa7Tx+Xc86tNQCH91D7GGBCeuN0OkEXEZGqItjk7sZppBzLzEL2OHm/36MGHiGF+Esp0aN0qAJbTIMPBgrKX0oBy8BFR02gXNjUWp+mHymnlFprkD0q2HuPURAIxspejT462hzFRU3pGBeh2TNXV+mD6FAPpI4epWN6YWhN6S7Fz8wx2AEEz6AlCmJmk06WSKNe4BoxmBkUpZS2bUPi0azfgK6Jil4spdRaJ2M0GJ0Gxf3vBNM6dDptIY+pU66QAghQ4No4QM8AP0BKP/99k57Ow/QG6Z/Xjfj/u9MvImH03GBoSbUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five = Image.open('five.jpg')\n",
    "five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b5b6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = five.convert(\"L\")\n",
    "\n",
    "# Save the grayscale image\n",
    "image.save(\"grayscale_five.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80b72dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_grey = Image.open('grayscale_five.jpg')\n",
    "arr = np.asarray(five_grey)\n",
    "arr = arr.reshape((1,784))\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0aed31c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2352 into shape (1,784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7952\\3684711908.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grayscale_five.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'grayscale_five.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2352 into shape (1,784)"
     ]
    }
   ],
   "source": [
    "five = Image.open('five.jpg')\n",
    "five.save(\"grayscale_five.jpg\")\n",
    "\n",
    "np.argmax(model.predict(np.asarray(Image.open('grayscale_five.jpg')).reshape(1,784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c57bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
